{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "You can manually switch to a GPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. You should do this before running the following cells to import packages, since the kernel gets restarted upon switching runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs682/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=128, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs682/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=128, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs682/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening:  tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2706, -1.5399,  1.2709,  0.7012,  0.3560],\n",
       "        [-0.1240,  0.6793, -2.1753,  0.6715, -1.5849],\n",
       "        [ 0.6144,  0.3046,  0.8502, -0.5951, -0.4138]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# create a weight of shape [3 x 5]\n",
    "# you should see the type `torch.cuda.FloatTensor` if you use GPU. \n",
    "# Otherwise it should be `torch.FloatTensor`\n",
    "random_weight((3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part2(loader, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: A DataLoader for the data split we want to check\n",
    "    - model_fn: A function that performs the forward pass of the model,\n",
    "      with the signature scores = model_fn(x, params)\n",
    "    - params: List of PyTorch Tensors giving parameters of the model\n",
    "    \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            scores = model_fn(x, params)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part2(model_fn, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        scores = model_fn(x, params)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "        # graph has requires_grad=True and uses backpropagation to compute the\n",
    "        # gradient of the loss with respect to these Tensors, and stores the\n",
    "        # gradients in the .grad attribute of each Tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            check_accuracy_part2(loader_val, model_fn, params)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [100/383] Loss: 1.6817\n",
      "Epoch [1/150], Step [200/383] Loss: 1.4033\n",
      "Epoch [1/150], Step [300/383] Loss: 1.1861\n",
      "Accuracy of the model on the test images: 53.33 %\n",
      "Epoch [2/150], Step [100/383] Loss: 1.0870\n",
      "Epoch [2/150], Step [200/383] Loss: 1.1575\n",
      "Epoch [2/150], Step [300/383] Loss: 1.1044\n",
      "Accuracy of the model on the test images: 56.47 %\n",
      "Epoch [3/150], Step [100/383] Loss: 0.9280\n",
      "Epoch [3/150], Step [200/383] Loss: 0.8235\n",
      "Epoch [3/150], Step [300/383] Loss: 1.0988\n",
      "Accuracy of the model on the test images: 61.67 %\n",
      "Epoch [4/150], Step [100/383] Loss: 0.9380\n",
      "Epoch [4/150], Step [200/383] Loss: 0.7304\n",
      "Epoch [4/150], Step [300/383] Loss: 0.8498\n",
      "Accuracy of the model on the test images: 64.86 %\n",
      "Epoch [5/150], Step [100/383] Loss: 0.7278\n",
      "Epoch [5/150], Step [200/383] Loss: 0.8408\n",
      "Epoch [5/150], Step [300/383] Loss: 0.7308\n",
      "Accuracy of the model on the test images: 66.39 %\n",
      "Epoch [6/150], Step [100/383] Loss: 0.6158\n",
      "Epoch [6/150], Step [200/383] Loss: 0.7844\n",
      "Epoch [6/150], Step [300/383] Loss: 0.6428\n",
      "Accuracy of the model on the test images: 67.83 %\n",
      "Epoch [7/150], Step [100/383] Loss: 0.6540\n",
      "Epoch [7/150], Step [200/383] Loss: 0.5634\n",
      "Epoch [7/150], Step [300/383] Loss: 0.5042\n",
      "Accuracy of the model on the test images: 66.69 %\n",
      "Epoch [8/150], Step [100/383] Loss: 0.5301\n",
      "Epoch [8/150], Step [200/383] Loss: 0.3749\n",
      "Epoch [8/150], Step [300/383] Loss: 0.5272\n",
      "Accuracy of the model on the test images: 68.79 %\n",
      "Epoch [9/150], Step [100/383] Loss: 0.2896\n",
      "Epoch [9/150], Step [200/383] Loss: 0.4044\n",
      "Epoch [9/150], Step [300/383] Loss: 0.3715\n",
      "Accuracy of the model on the test images: 68.68 %\n",
      "Epoch [10/150], Step [100/383] Loss: 0.3020\n",
      "Epoch [10/150], Step [200/383] Loss: 0.3997\n",
      "Epoch [10/150], Step [300/383] Loss: 0.3355\n",
      "Accuracy of the model on the test images: 68.52 %\n",
      "Epoch [11/150], Step [100/383] Loss: 0.2132\n",
      "Epoch [11/150], Step [200/383] Loss: 0.3110\n",
      "Epoch [11/150], Step [300/383] Loss: 0.3277\n",
      "Accuracy of the model on the test images: 67.23 %\n",
      "Epoch [12/150], Step [100/383] Loss: 0.2391\n",
      "Epoch [12/150], Step [200/383] Loss: 0.4683\n",
      "Epoch [12/150], Step [300/383] Loss: 0.2356\n",
      "Accuracy of the model on the test images: 69.02 %\n",
      "Epoch [13/150], Step [100/383] Loss: 0.1887\n",
      "Epoch [13/150], Step [200/383] Loss: 0.2249\n",
      "Epoch [13/150], Step [300/383] Loss: 0.2753\n",
      "Accuracy of the model on the test images: 68.39 %\n",
      "Epoch [14/150], Step [100/383] Loss: 0.0704\n",
      "Epoch [14/150], Step [200/383] Loss: 0.1766\n",
      "Epoch [14/150], Step [300/383] Loss: 0.1725\n",
      "Accuracy of the model on the test images: 69.57 %\n",
      "Epoch [15/150], Step [100/383] Loss: 0.0869\n",
      "Epoch [15/150], Step [200/383] Loss: 0.1415\n",
      "Epoch [15/150], Step [300/383] Loss: 0.1522\n",
      "Accuracy of the model on the test images: 69.42 %\n",
      "Epoch [16/150], Step [100/383] Loss: 0.1173\n",
      "Epoch [16/150], Step [200/383] Loss: 0.1221\n",
      "Epoch [16/150], Step [300/383] Loss: 0.0800\n",
      "Accuracy of the model on the test images: 68.87 %\n",
      "Epoch [17/150], Step [100/383] Loss: 0.1504\n",
      "Epoch [17/150], Step [200/383] Loss: 0.1471\n",
      "Epoch [17/150], Step [300/383] Loss: 0.1762\n",
      "Accuracy of the model on the test images: 69.63 %\n",
      "Epoch [18/150], Step [100/383] Loss: 0.1235\n",
      "Epoch [18/150], Step [200/383] Loss: 0.1164\n",
      "Epoch [18/150], Step [300/383] Loss: 0.1814\n",
      "Accuracy of the model on the test images: 69.57 %\n",
      "Epoch [19/150], Step [100/383] Loss: 0.0258\n",
      "Epoch [19/150], Step [200/383] Loss: 0.1746\n",
      "Epoch [19/150], Step [300/383] Loss: 0.0572\n",
      "Accuracy of the model on the test images: 69.17 %\n",
      "Epoch [20/150], Step [100/383] Loss: 0.0800\n",
      "Epoch [20/150], Step [200/383] Loss: 0.0538\n",
      "Epoch [20/150], Step [300/383] Loss: 0.1558\n",
      "Accuracy of the model on the test images: 69.38 %\n",
      "Epoch [21/150], Step [100/383] Loss: 0.0171\n",
      "Epoch [21/150], Step [200/383] Loss: 0.0936\n",
      "Epoch [21/150], Step [300/383] Loss: 0.1374\n",
      "Accuracy of the model on the test images: 69.45 %\n",
      "Epoch [22/150], Step [100/383] Loss: 0.1635\n",
      "Epoch [22/150], Step [200/383] Loss: 0.0833\n",
      "Epoch [22/150], Step [300/383] Loss: 0.0592\n",
      "Accuracy of the model on the test images: 69.63 %\n",
      "Epoch [23/150], Step [100/383] Loss: 0.0281\n",
      "Epoch [23/150], Step [200/383] Loss: 0.0324\n",
      "Epoch [23/150], Step [300/383] Loss: 0.0785\n",
      "Accuracy of the model on the test images: 69.85 %\n",
      "Epoch [24/150], Step [100/383] Loss: 0.0289\n",
      "Epoch [24/150], Step [200/383] Loss: 0.0560\n",
      "Epoch [24/150], Step [300/383] Loss: 0.0695\n",
      "Accuracy of the model on the test images: 69.6 %\n",
      "Epoch [25/150], Step [100/383] Loss: 0.0528\n",
      "Epoch [25/150], Step [200/383] Loss: 0.0349\n",
      "Epoch [25/150], Step [300/383] Loss: 0.0932\n",
      "Accuracy of the model on the test images: 70.31 %\n",
      "Epoch [26/150], Step [100/383] Loss: 0.0256\n",
      "Epoch [26/150], Step [200/383] Loss: 0.0745\n",
      "Epoch [26/150], Step [300/383] Loss: 0.0639\n",
      "Accuracy of the model on the test images: 70.87 %\n",
      "Epoch [27/150], Step [100/383] Loss: 0.0523\n",
      "Epoch [27/150], Step [200/383] Loss: 0.0659\n",
      "Epoch [27/150], Step [300/383] Loss: 0.0538\n",
      "Accuracy of the model on the test images: 70.09 %\n",
      "Epoch [28/150], Step [100/383] Loss: 0.0744\n",
      "Epoch [28/150], Step [200/383] Loss: 0.0279\n",
      "Epoch [28/150], Step [300/383] Loss: 0.0421\n",
      "Accuracy of the model on the test images: 69.93 %\n",
      "Epoch [29/150], Step [100/383] Loss: 0.0703\n",
      "Epoch [29/150], Step [200/383] Loss: 0.0364\n",
      "Epoch [29/150], Step [300/383] Loss: 0.0088\n",
      "Accuracy of the model on the test images: 70.33 %\n",
      "Epoch [30/150], Step [100/383] Loss: 0.0266\n",
      "Epoch [30/150], Step [200/383] Loss: 0.0451\n",
      "Epoch [30/150], Step [300/383] Loss: 0.0261\n",
      "Accuracy of the model on the test images: 70.14 %\n",
      "Epoch [31/150], Step [100/383] Loss: 0.0349\n",
      "Epoch [31/150], Step [200/383] Loss: 0.0670\n",
      "Epoch [31/150], Step [300/383] Loss: 0.0719\n",
      "Accuracy of the model on the test images: 70.18 %\n",
      "Epoch [32/150], Step [100/383] Loss: 0.0212\n",
      "Epoch [32/150], Step [200/383] Loss: 0.0124\n",
      "Epoch [32/150], Step [300/383] Loss: 0.0063\n",
      "Accuracy of the model on the test images: 70.72 %\n",
      "Epoch [33/150], Step [100/383] Loss: 0.0091\n",
      "Epoch [33/150], Step [200/383] Loss: 0.0059\n",
      "Epoch [33/150], Step [300/383] Loss: 0.0658\n",
      "Accuracy of the model on the test images: 70.06 %\n",
      "Epoch [34/150], Step [100/383] Loss: 0.0636\n",
      "Epoch [34/150], Step [200/383] Loss: 0.0113\n",
      "Epoch [34/150], Step [300/383] Loss: 0.0519\n",
      "Accuracy of the model on the test images: 70.55 %\n",
      "Epoch [35/150], Step [100/383] Loss: 0.0036\n",
      "Epoch [35/150], Step [200/383] Loss: 0.0202\n",
      "Epoch [35/150], Step [300/383] Loss: 0.1148\n",
      "Accuracy of the model on the test images: 71.07 %\n",
      "Epoch [36/150], Step [100/383] Loss: 0.1168\n",
      "Epoch [36/150], Step [200/383] Loss: 0.0036\n",
      "Epoch [36/150], Step [300/383] Loss: 0.0473\n",
      "Accuracy of the model on the test images: 71.51 %\n",
      "Epoch [37/150], Step [100/383] Loss: 0.0047\n",
      "Epoch [37/150], Step [200/383] Loss: 0.0113\n",
      "Epoch [37/150], Step [300/383] Loss: 0.0608\n",
      "Accuracy of the model on the test images: 70.59 %\n",
      "Epoch [38/150], Step [100/383] Loss: 0.0270\n",
      "Epoch [38/150], Step [200/383] Loss: 0.0790\n",
      "Epoch [38/150], Step [300/383] Loss: 0.0314\n",
      "Accuracy of the model on the test images: 70.75 %\n",
      "Epoch [39/150], Step [100/383] Loss: 0.0367\n",
      "Epoch [39/150], Step [200/383] Loss: 0.0029\n",
      "Epoch [39/150], Step [300/383] Loss: 0.0428\n",
      "Accuracy of the model on the test images: 70.04 %\n",
      "Epoch [40/150], Step [100/383] Loss: 0.0290\n",
      "Epoch [40/150], Step [200/383] Loss: 0.0107\n",
      "Epoch [40/150], Step [300/383] Loss: 0.0649\n",
      "Accuracy of the model on the test images: 71.31 %\n",
      "Epoch [41/150], Step [100/383] Loss: 0.0181\n",
      "Epoch [41/150], Step [200/383] Loss: 0.0442\n",
      "Epoch [41/150], Step [300/383] Loss: 0.0123\n",
      "Accuracy of the model on the test images: 70.73 %\n",
      "Epoch [42/150], Step [100/383] Loss: 0.0280\n",
      "Epoch [42/150], Step [200/383] Loss: 0.0010\n",
      "Epoch [42/150], Step [300/383] Loss: 0.0484\n",
      "Accuracy of the model on the test images: 70.72 %\n",
      "Epoch [43/150], Step [100/383] Loss: 0.0727\n",
      "Epoch [43/150], Step [200/383] Loss: 0.0065\n",
      "Epoch [43/150], Step [300/383] Loss: 0.0121\n",
      "Accuracy of the model on the test images: 70.35 %\n",
      "Epoch [44/150], Step [100/383] Loss: 0.0025\n",
      "Epoch [44/150], Step [200/383] Loss: 0.0028\n",
      "Epoch [44/150], Step [300/383] Loss: 0.0089\n",
      "Accuracy of the model on the test images: 70.45 %\n",
      "Epoch [45/150], Step [100/383] Loss: 0.0235\n",
      "Epoch [45/150], Step [200/383] Loss: 0.0032\n",
      "Epoch [45/150], Step [300/383] Loss: 0.0354\n",
      "Accuracy of the model on the test images: 70.97 %\n",
      "Epoch [46/150], Step [100/383] Loss: 0.0320\n",
      "Epoch [46/150], Step [200/383] Loss: 0.0072\n",
      "Epoch [46/150], Step [300/383] Loss: 0.0049\n",
      "Accuracy of the model on the test images: 71.25 %\n",
      "Epoch [47/150], Step [100/383] Loss: 0.0768\n",
      "Epoch [47/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [47/150], Step [300/383] Loss: 0.0283\n",
      "Accuracy of the model on the test images: 70.59 %\n",
      "Epoch [48/150], Step [100/383] Loss: 0.0020\n",
      "Epoch [48/150], Step [200/383] Loss: 0.0065\n",
      "Epoch [48/150], Step [300/383] Loss: 0.0046\n",
      "Accuracy of the model on the test images: 70.28 %\n",
      "Epoch [49/150], Step [100/383] Loss: 0.0217\n",
      "Epoch [49/150], Step [200/383] Loss: 0.0370\n",
      "Epoch [49/150], Step [300/383] Loss: 0.0015\n",
      "Accuracy of the model on the test images: 71.08 %\n",
      "Epoch [50/150], Step [100/383] Loss: 0.0068\n",
      "Epoch [50/150], Step [200/383] Loss: 0.0012\n",
      "Epoch [50/150], Step [300/383] Loss: 0.0157\n",
      "Accuracy of the model on the test images: 70.66 %\n",
      "Epoch [51/150], Step [100/383] Loss: 0.0382\n",
      "Epoch [51/150], Step [200/383] Loss: 0.0200\n",
      "Epoch [51/150], Step [300/383] Loss: 0.0008\n",
      "Accuracy of the model on the test images: 71.51 %\n",
      "Epoch [52/150], Step [100/383] Loss: 0.0176\n",
      "Epoch [52/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [52/150], Step [300/383] Loss: 0.0007\n",
      "Accuracy of the model on the test images: 70.87 %\n",
      "Epoch [53/150], Step [100/383] Loss: 0.0419\n",
      "Epoch [53/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [53/150], Step [300/383] Loss: 0.0933\n",
      "Accuracy of the model on the test images: 70.98 %\n",
      "Epoch [54/150], Step [100/383] Loss: 0.0134\n",
      "Epoch [54/150], Step [200/383] Loss: 0.0033\n",
      "Epoch [54/150], Step [300/383] Loss: 0.0014\n",
      "Accuracy of the model on the test images: 71.29 %\n",
      "Epoch [55/150], Step [100/383] Loss: 0.0127\n",
      "Epoch [55/150], Step [200/383] Loss: 0.0139\n",
      "Epoch [55/150], Step [300/383] Loss: 0.0370\n",
      "Accuracy of the model on the test images: 70.76 %\n",
      "Epoch [56/150], Step [100/383] Loss: 0.0050\n",
      "Epoch [56/150], Step [200/383] Loss: 0.0285\n",
      "Epoch [56/150], Step [300/383] Loss: 0.0021\n",
      "Accuracy of the model on the test images: 71.31 %\n",
      "Epoch [57/150], Step [100/383] Loss: 0.0020\n",
      "Epoch [57/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [57/150], Step [300/383] Loss: 0.0537\n",
      "Accuracy of the model on the test images: 71.02 %\n",
      "Epoch [58/150], Step [100/383] Loss: 0.0005\n",
      "Epoch [58/150], Step [200/383] Loss: 0.0047\n",
      "Epoch [58/150], Step [300/383] Loss: 0.0020\n",
      "Accuracy of the model on the test images: 71.41 %\n",
      "Epoch [59/150], Step [100/383] Loss: 0.0291\n",
      "Epoch [59/150], Step [200/383] Loss: 0.0010\n",
      "Epoch [59/150], Step [300/383] Loss: 0.0047\n",
      "Accuracy of the model on the test images: 71.18 %\n",
      "Epoch [60/150], Step [100/383] Loss: 0.0028\n",
      "Epoch [60/150], Step [200/383] Loss: 0.0012\n",
      "Epoch [60/150], Step [300/383] Loss: 0.0259\n",
      "Accuracy of the model on the test images: 71.24 %\n",
      "Epoch [61/150], Step [100/383] Loss: 0.0073\n",
      "Epoch [61/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [61/150], Step [300/383] Loss: 0.0146\n",
      "Accuracy of the model on the test images: 71.1 %\n",
      "Epoch [62/150], Step [100/383] Loss: 0.0085\n",
      "Epoch [62/150], Step [200/383] Loss: 0.0007\n",
      "Epoch [62/150], Step [300/383] Loss: 0.0157\n",
      "Accuracy of the model on the test images: 71.28 %\n",
      "Epoch [63/150], Step [100/383] Loss: 0.0127\n",
      "Epoch [63/150], Step [200/383] Loss: 0.0019\n",
      "Epoch [63/150], Step [300/383] Loss: 0.0179\n",
      "Accuracy of the model on the test images: 70.87 %\n",
      "Epoch [64/150], Step [100/383] Loss: 0.0085\n",
      "Epoch [64/150], Step [200/383] Loss: 0.0188\n",
      "Epoch [64/150], Step [300/383] Loss: 0.0209\n",
      "Accuracy of the model on the test images: 71.51 %\n",
      "Epoch [65/150], Step [100/383] Loss: 0.0012\n",
      "Epoch [65/150], Step [200/383] Loss: 0.0018\n",
      "Epoch [65/150], Step [300/383] Loss: 0.0116\n",
      "Accuracy of the model on the test images: 71.31 %\n",
      "Epoch [66/150], Step [100/383] Loss: 0.0266\n",
      "Epoch [66/150], Step [200/383] Loss: 0.0002\n",
      "Epoch [66/150], Step [300/383] Loss: 0.0010\n",
      "Accuracy of the model on the test images: 71.36 %\n",
      "Epoch [67/150], Step [100/383] Loss: 0.0006\n",
      "Epoch [67/150], Step [200/383] Loss: 0.0087\n",
      "Epoch [67/150], Step [300/383] Loss: 0.0007\n",
      "Accuracy of the model on the test images: 71.64 %\n",
      "Epoch [68/150], Step [100/383] Loss: 0.0004\n",
      "Epoch [68/150], Step [200/383] Loss: 0.0172\n",
      "Epoch [68/150], Step [300/383] Loss: 0.0298\n",
      "Accuracy of the model on the test images: 71.27 %\n",
      "Epoch [69/150], Step [100/383] Loss: 0.0004\n",
      "Epoch [69/150], Step [200/383] Loss: 0.0001\n",
      "Epoch [69/150], Step [300/383] Loss: 0.0084\n",
      "Accuracy of the model on the test images: 71.35 %\n",
      "Epoch [70/150], Step [100/383] Loss: 0.0144\n",
      "Epoch [70/150], Step [200/383] Loss: 0.0020\n",
      "Epoch [70/150], Step [300/383] Loss: 0.0030\n",
      "Accuracy of the model on the test images: 71.61 %\n",
      "Epoch [71/150], Step [100/383] Loss: 0.0023\n",
      "Epoch [71/150], Step [200/383] Loss: 0.0002\n",
      "Epoch [71/150], Step [300/383] Loss: 0.0010\n",
      "Accuracy of the model on the test images: 71.36 %\n",
      "Epoch [72/150], Step [100/383] Loss: 0.0003\n",
      "Epoch [72/150], Step [200/383] Loss: 0.0010\n",
      "Epoch [72/150], Step [300/383] Loss: 0.0040\n",
      "Accuracy of the model on the test images: 70.79 %\n",
      "Epoch [73/150], Step [100/383] Loss: 0.0068\n",
      "Epoch [73/150], Step [200/383] Loss: 0.0023\n",
      "Epoch [73/150], Step [300/383] Loss: 0.0027\n",
      "Accuracy of the model on the test images: 71.69 %\n",
      "Epoch [74/150], Step [100/383] Loss: 0.0033\n",
      "Epoch [74/150], Step [200/383] Loss: 0.0002\n",
      "Epoch [74/150], Step [300/383] Loss: 0.0001\n",
      "Accuracy of the model on the test images: 71.96 %\n",
      "Epoch [75/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [75/150], Step [200/383] Loss: 0.0001\n",
      "Epoch [75/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 72.97 %\n",
      "Epoch [76/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [76/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [76/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.12 %\n",
      "Epoch [77/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [77/150], Step [200/383] Loss: 0.0001\n",
      "Epoch [77/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.14 %\n",
      "Epoch [78/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [78/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [78/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.15 %\n",
      "Epoch [79/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [79/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [79/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.15 %\n",
      "Epoch [80/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [80/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [80/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.19 %\n",
      "Epoch [81/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [81/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [81/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.17 %\n",
      "Epoch [82/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [82/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [82/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.17 %\n",
      "Epoch [83/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [83/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [83/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.16 %\n",
      "Epoch [84/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [84/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [84/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.19 %\n",
      "Epoch [85/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [85/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [85/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.16 %\n",
      "Epoch [86/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [86/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [86/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.2 %\n",
      "Epoch [87/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [87/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [87/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.23 %\n",
      "Epoch [88/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [88/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [88/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.24 %\n",
      "Epoch [89/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [89/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [89/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.24 %\n",
      "Epoch [90/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [90/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [90/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [91/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [91/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [91/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.28 %\n",
      "Epoch [92/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [92/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [92/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.28 %\n",
      "Epoch [93/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [93/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [93/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [94/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [94/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [94/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.27 %\n",
      "Epoch [95/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [95/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [95/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [96/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [96/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [96/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.28 %\n",
      "Epoch [97/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [97/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [97/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [98/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [98/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [98/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.25 %\n",
      "Epoch [99/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [99/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [99/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [100/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [100/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [100/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.25 %\n",
      "Epoch [101/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [101/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [101/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.25 %\n",
      "Epoch [102/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [102/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [102/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [103/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [103/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [103/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.25 %\n",
      "Epoch [104/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [104/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [104/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.24 %\n",
      "Epoch [105/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [105/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [105/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.24 %\n",
      "Epoch [106/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [106/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [106/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.23 %\n",
      "Epoch [107/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [107/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [107/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.24 %\n",
      "Epoch [108/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [108/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [108/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.23 %\n",
      "Epoch [109/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [109/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [109/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.23 %\n",
      "Epoch [110/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [110/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [110/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.23 %\n",
      "Epoch [111/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [111/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [111/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.25 %\n",
      "Epoch [112/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [112/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [112/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.26 %\n",
      "Epoch [113/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [113/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [113/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.27 %\n",
      "Epoch [114/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [114/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [114/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [115/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [115/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [115/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [116/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [116/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [116/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [117/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [117/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [117/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [118/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [118/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [118/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [119/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [119/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [119/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.27 %\n",
      "Epoch [120/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [120/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [120/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.27 %\n",
      "Epoch [121/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [121/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [121/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [122/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [122/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [122/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [123/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [123/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [123/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.3 %\n",
      "Epoch [124/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [124/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [124/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [125/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [125/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [125/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.28 %\n",
      "Epoch [126/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [126/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [126/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.28 %\n",
      "Epoch [127/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [127/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [127/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [128/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [128/150], Step [200/383] Loss: 0.0000\n",
      "Epoch [128/150], Step [300/383] Loss: 0.0000\n",
      "Accuracy of the model on the test images: 73.29 %\n",
      "Epoch [129/150], Step [100/383] Loss: 0.0000\n",
      "Epoch [129/150], Step [200/383] Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "num_epochs = 150\n",
    "batch_size = 100\n",
    "#learning_rate = 1e-4\n",
    "learning_decay = 0.95\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
    "learning_rate = 0.005\n",
    "model = resnet18(num_classes=10)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "# Train the model\n",
    "total_step = len(loader_train)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(loader_train):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    curr_lr = learning_rate*learning_decay\n",
    "    update_lr(optimizer, curr_lr)\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr = learning_rate*learning_decay\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loader_test:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "    scheduler.step()\n",
    "\n",
    "#train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe what you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 7361 / 10000 correct (73.61)\n"
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(loader_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
